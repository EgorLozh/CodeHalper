# CodeHalper

CodeHalper — это интеллектуальный помощник по коду, который помогает разработчикам лучше понимать и работать со своей кодовой базой. Он использует современные языковые модели и эмбеддинги, чтобы предоставлять помощь с учётом контекста для вопросов, связанных с кодом.

## Возможности

- Семантический поиск и понимание кода
- Поддержка нескольких языков программирования
- Интеллектуальная документация и объяснение кода
- Контекстно-зависимая помощь в написании кода
- Хранение векторов на основе FAISS для эффективного поиска

## Используемые технологии

- Python 3.11+
- LangChain для управления языковыми моделями (LLM)
- Ollama для локального запуска LLM и создания эмбеддингов
- FAISS для поиска похожих векторов
- Python-dotenv для управления переменными окружения

## Предварительные требования

- Python 3.11 или выше
- Установленный и запущенный локально Ollama
- Необходимые Python-пакеты (указаны в pyproject.toml)

## Начало работы

1. Склонируйте репозиторий:
```bash
git clone https://github.com/EgorLozh/CodeHalper.git
cd CodeHalper
```

2. Создайте и активируйте виртуальное окружение:
```bash
uv venv

source .venv/bin/activate   # на Unix/macOS
# или
.venv\Scripts\activate      # на Windows
```

3. Установите зависимости:
```bash
uv install -e .
```

4. Настройте проект:
   - Посмотрите файл `config.yaml` и измените настройки при необходимости
   - Убедитесь, что Ollama запущен и содержит нужные модели

## Конфигурация (config.yaml)

Файл `config.yaml` содержит несколько важных разделов конфигурации:

### Модели
- `llm`: Указывает используемую языковую модель (по умолчанию: "deepseek-coder-v2")
- `embedding`: Указывает модель для создания эмбеддингов (по умолчанию: "nomic-embed-text")

### Обработка документов
- `available_types`: Список поддерживаемых расширений файлов
- `chunk_size`: Размер фрагментов текста для обработки (по умолчанию: 500)
- `chunk_overlap`: Перекрытие между фрагментами (по умолчанию: 50)

### Пути
- `db_dir`: Директория для хранения векторной базы данных FAISS

### Настройки поиска
- `template`: Шаблон запроса для языковой модели
- `default_k`: Количество извлекаемых документов (по умолчанию: 5)

## Использование

1. Убедитесь, что Ollama запущен и загружен нужными моделями:
```bash
ollama pull deepseek-coder-v2
ollama pull nomic-embed-text
ollama serve
```

2. Запустите приложение:
```bash
python uv run src/cli.py
```

